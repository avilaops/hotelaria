# ü§ñ Guia de Instala√ß√£o do Ollama

## O que √© Ollama?
Ollama √© uma ferramenta que permite executar modelos de IA (como Llama, Mistral, etc.) localmente no seu computador, sem precisar de internet ou servi√ßos na nuvem.

## üì• Instala√ß√£o

### Windows (Recomendado)
1. Baixe o instalador: https://ollama.com/download/windows
2. Execute o instalador `OllamaSetup.exe`
3. Siga as instru√ß√µes do instalador
4. Ollama ser√° instalado como servi√ßo do Windows

### Verificar Instala√ß√£o
Abra o PowerShell e execute:
```powershell
ollama --version
```

## üöÄ Baixar e Executar Modelos

### Modelo Recomendado: Llama 3.2 (3B)
Este √© um modelo leve e r√°pido, ideal para come√ßar:

```powershell
ollama pull llama3.2
```

### Testar o Modelo
```powershell
ollama run llama3.2 "Ol√°, como voc√™ pode me ajudar?"
```

### Outros Modelos Dispon√≠veis

#### Modelos Leves (at√© 8GB RAM)
- `llama3.2` (3B) - R√°pido e eficiente
- `phi3` (3.8B) - √ìtimo para racioc√≠nio
- `gemma2:2b` (2B) - Super leve

#### Modelos M√©dios (12-16GB RAM)
- `mistral` (7B) - Equilibrado
- `llama3.1:8b` (8B) - Vers√£o maior do Llama

#### Modelos Avan√ßados (32GB+ RAM)
- `llama3.1:70b` (70B) - Alta qualidade
- `mixtral` (47B) - Modelo expert mixture

### Baixar Modelo
```powershell
# Exemplo: baixar Mistral
ollama pull mistral
```

## üîß Configura√ß√£o no Sistema Hotelaria

### 1. Verificar se o Ollama est√° rodando
O Ollama roda automaticamente em `http://localhost:11434`

Teste no navegador:
```
http://localhost:11434/
```

Deve retornar: `Ollama is running`

### 2. Configurar vari√°veis de ambiente (opcional)

Crie ou edite o arquivo `.env` na raiz do projeto:

```env
# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2
```

### 3. Executar a aplica√ß√£o
```powershell
dotnet run
```

### 4. Acessar o Assistente IA
Navegue at√©: `http://localhost:5000/assistente-ia`

## üìä Uso no Sistema

### Funcionalidades Dispon√≠veis

#### 1. Chat Interativo üí¨
- Fa√ßa perguntas sobre gest√£o hoteleira
- Pe√ßa conselhos sobre ocupa√ß√£o
- Tire d√∫vidas sobre o sistema

#### 2. An√°lise de Ocupa√ß√£o üìä
- An√°lise autom√°tica do m√™s atual
- Identifica√ß√£o de tend√™ncias
- Recomenda√ß√µes de melhorias

#### 3. Sugest√£o de Pre√ßos üí∞
- Pre√ßos otimizados por quarto
- Considera sazonalidade e dia da semana
- Baseado em pr√°ticas de revenue management

#### 4. Relat√≥rio Financeiro üìà
- An√°lise de receita e despesas
- Avalia√ß√£o de margem de lucro
- Sugest√µes de otimiza√ß√£o

#### 5. Otimiza√ß√£o de Descri√ß√µes ‚úçÔ∏è
- Descri√ß√µes atrativas para quartos
- SEO-friendly
- Profissional e persuasiva

## üõ†Ô∏è Troubleshooting

### Ollama n√£o est√° respondendo
1. Verifique se o servi√ßo est√° rodando:
   ```powershell
   Get-Service | Where-Object {$_.DisplayName -like "*Ollama*"}
   ```

2. Reinicie o servi√ßo:
   ```powershell
   Restart-Service -Name "Ollama"
   ```

3. Ou reinicie manualmente:
   - Abra o menu Iniciar
   - Procure por "Ollama"
   - Clique com bot√£o direito ‚Üí "Executar como administrador"

### Modelo demora muito para responder
- Use um modelo mais leve (como `llama3.2`)
- Verifique se seu computador tem RAM suficiente
- Feche outros aplicativos pesados

### Erro "Connection refused"
- Verifique se Ollama est√° instalado
- Confirme que est√° rodando na porta 11434
- Teste: `curl http://localhost:11434/`

### Modelo retorna respostas estranhas
- O modelo pode n√£o estar em portugu√™s
- Tente adicionar ao prompt: "Responda sempre em portugu√™s de Portugal"
- Use um modelo multil√≠ngue como `mistral`

## üéØ Melhores Pr√°ticas

### Escolha do Modelo
- **Desenvolvimento/Testes**: `llama3.2` (r√°pido)
- **Produ√ß√£o**: `mistral` ou `llama3.1:8b` (melhor qualidade)
- **Hardware limitado**: `phi3` ou `gemma2:2b`

### Performance
- Primeira execu√ß√£o √© sempre mais lenta (carrega modelo)
- Mantenha Ollama rodando em background
- Use SSD para armazenar modelos (mais r√°pido)

### Prompts Eficientes
- Seja espec√≠fico no que pede
- Forne√ßa contexto relevante
- Use portugu√™s claro e direto

## üìö Recursos Adicionais

- **Site Oficial**: https://ollama.com
- **Modelos Dispon√≠veis**: https://ollama.com/library
- **Documenta√ß√£o API**: https://github.com/ollama/ollama/blob/main/docs/api.md
- **Comunidade**: https://discord.gg/ollama

## üîê Privacidade e Seguran√ßa

### Vantagens do Ollama
‚úÖ **100% Local** - Nenhum dado √© enviado para internet
‚úÖ **Privado** - Dados sens√≠veis ficam no seu computador
‚úÖ **Sem custos** - N√£o paga por tokens ou API calls
‚úÖ **Offline** - Funciona sem internet
‚úÖ **Controle total** - Voc√™ escolhe o modelo e configura√ß√µes

### Dados que ficam locais
- Perguntas do chat
- Dados de reservas analisados
- Informa√ß√µes financeiras
- Descri√ß√µes geradas

## üí° Dicas Avan√ßadas

### Usar modelo diferente para cada tarefa
Edite `Services/OllamaService.cs` e adicione:

```csharp
var modeloFinanceiro = "mistral";    // Melhor para an√°lises
var modeloChat = "llama3.2";         // R√°pido para chat
var modeloTexto = "phi3";            // √ìtimo para textos
```

### Ajustar timeout
No `Program.cs`, altere:
```csharp
client.Timeout = TimeSpan.FromMinutes(10); // Aumentar para modelos lentos
```

### Executar em GPU
Se tiver NVIDIA GPU, Ollama usa automaticamente.
Verifique no Task Manager ‚Üí Performance ‚Üí GPU

## ‚úÖ Checklist de Instala√ß√£o

- [ ] Ollama instalado (`ollama --version` funciona)
- [ ] Modelo baixado (`ollama list` mostra modelos)
- [ ] Ollama rodando (`http://localhost:11434` responde)
- [ ] `.env` configurado (opcional)
- [ ] Aplica√ß√£o compila (`dotnet build`)
- [ ] P√°gina carrega (`/assistente-ia` abre)
- [ ] Chat funciona (recebe respostas)

## üéì Pr√≥ximos Passos

1. Instale o Ollama
2. Baixe o modelo `llama3.2`
3. Execute a aplica√ß√£o
4. Acesse `/assistente-ia`
5. Teste as a√ß√µes r√°pidas
6. Experimente o chat

**Pronto para come√ßar!** üöÄ
